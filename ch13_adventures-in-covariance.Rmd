---
title: "Chapter 13. Adventures in Covariance"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      comment = "#>",
                      cache = TRUE,
                      dpi = 300)

library(glue)
library(mustashe)
library(magrittr)
library(broom)
library(patchwork)
library(MASS)
library(rethinking)
library(tidyverse)
library(conflicted)


conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("rename", "dplyr")

# To be able to use `map2stan()`:
conflict_prefer("collapse", "dplyr")
conflict_prefer("extract", "tidyr")
conflict_prefer("rstudent", "rethinking")
conflict_prefer("lag", "dplyr")
conflict_prefer("map", "purrr")
conflict_prefer("Position", "ggplot2")

# To be able to use 'MASS':
conflict_prefer("area", "patchwork")

theme_set(theme_minimal())
source_scripts()
set.seed(0)

# For 'rethinking'.
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

purple <- "#8067bf"
blue <- "#5896d1"
red <- "#d15858"
light_grey <- "grey80"
grey <- "grey50"
dark_grey <- "grey25"

# To shut-up `summarise()` in dplyr 1.0.0
options(dplyr.summarise.inform = FALSE)
```

- this chapter will show how to specify *varying slopes* in combination with varying intercepts
    * enables pooling to improve estimates of how different units respond to or are influenced by predictor variables
    * also improves the estimates of intercepts by "borrowing information across parameter types"
    * "varying slope models are massive interaction machines"

## 13.1 Varying slopes by construction

- pool information across intercepts and slopes by modeling the joint population of intercepts and slopes
    * modeling their covariance
    * assigning a 2D Gaussian distribution to both the intercepts (first dimension) and the slopes (second dimension)
- the variance-covariance matrix for a fit model describes how each parameters posterior probability is associated with one another
    * varying intercepts have variation, varying slopes have variation, and intercepts and slopes covary
- use example of visiting coffee shops:
    * visit different cafes, order a coffee, and record the wait time
        - previously, used varying intercepts, one for each cafe
    * also record the time of day
        - the average wait time is longer in the mornings than afternoons because they are busier in the mornings
    * different cafes vary in their average wait times and their differences between morning and afternoon
        - the differences in wait time by time of day are the slopes
    * cafes covary in their intercepts and slopes
        - because the popular cafes have much longer wait times in the morning leading to large differences between morning and afternoon

$$
\mu_i = \alpha_{\text{cafe}[i]} + \beta_{\text{cafe}[i]} A_i
$$

## 13.1.1 Simulate the population

- define the population of cafes
    * define the average wait time in the morning and afternoon
    * define the correlation between them

```{r}
a <- 3.5        # average morning wait time
b <- -1         # average difference afternoon wait time
sigma_a <- 1    # standard deviation in intercepts
sigma_b <- 0.5  # standard deviation in slopes
rho <- -0.7     # correlation between intercepts and slopes
```

- use these values to simulate a sample of cafes
    * define the multivariate Gaussian with a vector of means and a 2x2 matrix of variances and covariances

```{r}
Mu <- c(a, b)  # vector of two means
```

- the matrix of variances and covariances is arranged as follows

$$
\begin{pmatrix}
\sigma_\alpha^2 & \sigma_\alpha \sigma_\beta \rho \\
\sigma_\alpha \sigma_\beta \rho & \sigma_\beta^2
\end{pmatrix}
$$

- can construct the matrix explicitly

```{r}
cov_ab <- sigma_a * sigma_b * rho
Sigma <- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), nrow = 2)
Sigma
```

- another way to build the variance-covariance matrix using matrix multiplication
    * this is likely a better approach with larger models

```{r}
sigmas <- c(sigma_a, sigma_b)
Rho <- matrix(c(1, rho, rho, 1), nrow = 2)
Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)
Sigma
```

- now can simulate 20 cafes, each with their own intercept and slope

```{r}
N_cafes <- 20
```

- simulate from a multivariate Gaussian using `mvnorm()` from the 'MASS' package
    * returns a matrix of $\text{cafe} \times (\text{intercept}, \text{slope})$

```{r}
library(MASS)

set.seed(5)
vary_effects <- mvrnorm(n = N_cafes, mu = Mu, Sigma = Sigma)
head(vary_effects)
```

```{r}
# Split into separate vectors for ease of use later.
a_cafe <- vary_effects[, 1]
b_cafe <- vary_effects[, 2]
cor(a_cafe, b_cafe)
```

- plot of the varying effects

```{r}
as.data.frame(vary_effects) %>%
    as_tibble() %>%
    set_names(c("intercept", "slope")) %>%
    ggplot(aes(x = intercept, y = slope)) +
    geom_point()
```

### 13.1.2 Simulate observations

- simulate the visits to each cafe
    * 10 visits to each cafe, 5 in the morning and 5 in the afternoon

```{r}
N_visits <- 10
afternoon <- rep(0:1, N_visits * N_cafes / 2)
cafe_id <- rep(1:N_cafes, each = N_visits)

# Get the average wait time for each cafe in the morning and afternoon.
mu <- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon

sigma <- 0.5  # Standard deviation within cafes

# Sample wait times with each cafes unique average wait time per time of day.
wait_times <- rnorm(N_visits*N_cafes, mu, sigma)

d <- tibble(cafe = cafe_id, afternoon, wait = wait_times)
d
```

```{r}
d %>%
    ggplot(aes(x = wait, y = afternoon, color = factor(cafe))) +
    geom_jitter(width = 0, height = 0.2) +
    scale_color_manual(values = randomcoloR::distinctColorPalette(N_cafes),
                       guide = FALSE)
```

```{r}
d %>%
    ggplot(aes(x = wait, y = factor(cafe), color = factor(afternoon))) +
    geom_point() +
    scale_color_manual(values = c(blue, red))
```

### 13.1.3 The varying slopes model

- model with varying intercepts and slopes (explanation follows)

$$
W_i \sim \text{Normal}(\mu_i, \sigma) \\

\mu_i = \alpha_{\text{cafe}[i]} + \beta_{\text{cafe}[i]} A_i \\

\begin{bmatrix}
    \alpha_\text{cafe} \\
    \beta_\text{cafe}
\end{bmatrix} \sim \text{MVNormal}(
    \begin{bmatrix} \alpha \\ \beta \end{bmatrix}, \textbf{S}
) \\

\textbf{S} = 
\begin{pmatrix} 
    \sigma_\alpha & 0 \\ 0 &\sigma_\beta
\end{pmatrix}
\textbf{R}
\begin{pmatrix} 
    \sigma_\alpha & 0 \\ 0 &\sigma_\beta
\end{pmatrix} \\

\alpha \sim \text{Normal}(0,10) \\
\beta \sim \text{Normal}(0,10) \\
\sigma \sim \text{HalfCauchy}(0, 1) \\
\sigma_\alpha \sim \text{HalfCauchy}(0, 1) \\
\sigma_\beta \sim \text{HalfCauchy}(0, 1) \\
\textbf{R} \sim \text{LKJcorr}(2)
$$

- the third like defines the population of varying intercepts and slopes        
    * each cafe has an intercept and slope with a prior distribution defined by the 2D Gaussian distribution with means $\alpha$ and $\beta$ and covariance matrix $\text{S}$

$$
\begin{bmatrix}
    \alpha_\text{cafe} \\
    \beta_\text{cafe}
\end{bmatrix} \sim \text{MVNormal}(
    \begin{bmatrix} \alpha \\ \beta \end{bmatrix}, \textbf{S}
)
$$

- the next line defines the variance-covariance matrix $\textbf{S}$
    * factoring it into simple standard deviations $\sigma_\alpha$ and $\sigma_\beta$ and a correlation matrix $\textbf{R}$
    * there are other ways to do this, but this formulation helps understand the inferred structure of the varying effects

$$
\textbf{S} = 
\begin{pmatrix} 
    \sigma_\alpha & 0 \\ 0 &\sigma_\beta
\end{pmatrix}
\textbf{R}
\begin{pmatrix} 
    \sigma_\alpha & 0 \\ 0 &\sigma_\beta
\end{pmatrix}
$$

- the correlation matrix has a prior defined as $\textbf{R} \sim \text{LKJcorr}(2)$
    * the correlation matrix will have the structure: $\begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}$ where $\rho$ is the correlation between the intercepts and slopes
    * with additional varying slopes, there are more correlation parameters, but the $\text{LKJcorr}$ prior will still work
    * the $\text{LKJcorr}(2)$ prior defines a weakly informative prior on $\rho$ that is skeptical of extreme correlations near -1 and 1
    * it has a single parameter $\eta$ that controls how "skeptical" the prior is of large correlations
        - if $\eta=1$, the prior is flat from -1 to 1
        - a large value of $\eta$ the mass of the distribution moves towards 0

```{r}
tibble(eta = c(1, 2, 4)) %>%
    mutate(value = map(eta, ~ rlkjcorr(1e5, K = 2, eta = .x)[, 1, 2])) %>%
    unnest(value) %>%
    ggplot(aes(x = value)) +
    geom_density(aes(color = factor(eta), fill = factor(eta)), 
                 size = 1.3, alpha = 0.1) +
    scale_color_brewer(palette = "Set2") +
    scale_fill_brewer(palette = "Set2") +
    labs(x = "correlation",
         y = "density",
         title = "Distributions from LKJcorr for different scale values",
         color = "eta", fill = "eta")
```

- now can fit the model

```{r}
stash("m13_1", {
    m13_1 <- map2stan(
        alist(
            wait ~ dnorm(mu, sigma),
            mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,
            c(a_cafe, b_cafe)[cafe] ~ dmvnorm2(c(a, b), sigma_cafe, Rho),
            a ~ dnorm(0, 10),
            b ~ dnorm(0, 10),
            sigma_cafe ~ dcauchy(0, 2),
            sigma ~ dcauchy(0, 2),
            Rho ~ dlkjcorr(2)
        ),
        data = d,
        iter = 5e3, warmup = 2e3, chains = 2
    )
})

precis(m13_1, depth = 1)
```

- inspection of the posterior distribution of varying effects
    * start with the posterior correlation between intercepts and slopes
        - the posterior distribution of the correlation between varying effects is decidedly negative

```{r}
post <- extract.samples(m13_1)

tribble(
    ~name, ~value,
    "posterior", post$Rho[, 1, 2],
    "prior", rlkjcorr(1e5, K = 2, eta = 2)[, 1, 2]
) %>%
    unnest(value) %>%
    ggplot(aes(x = value, color = name, fill = name)) +
    geom_density(size = 1.3, alpha = 0.2) +
    scale_color_manual(values = c(blue, grey)) +
    scale_fill_manual(values = c(blue, grey)) +
    theme(legend.title = element_blank(),
          legend.position = c(0.85, 0.5)) +
    labs(x = "correlation",
         y = "probability density",
         title = "Varying effect correlation posterior distribution")
```

- consider the shrinkage
    * the inferred correlation between varying effects pooled information across them
    * and the inferred variation within each varying effect was pooled
    * together the variances and correlation define a multivariate Gaussian prior for the varying effects
    * this prior regularizes the intercepts and slopes
- plot the posterior mean varying effects
    * compare them to the raw, unpooled estimates
    * also plot the inferred prior for the population of intercepts and slopes

> There is something wrong with the following 2 plots, but I cannot figure out what went wrong.

```{r}
# Raw, unpooled estimates for alpha and beta.
a1 <- map_dbl(1:N_cafes, function(i) {
    mean(d$wait[d$cafe == i & d$afternoon == 0])
})

b1 <- map_dbl(1:N_cafes, function(i) {
    mean(d$wait[d$cafe == i & d$afternoon == 1])
})
b1 <- b1 - a1

# Extract posterior means of partially pooled estimates.
post <- extract.samples(m13_1)
a2 <- apply(post$a_cafe, 2, mean)
b2 <- apply(post$b_cafe, 2, mean)

tribble(
    ~ name, ~ a, ~ b,
    "unpooled", a1, b1,
    "pooled", a2, b2
) %>%
    unnest(c(a, b)) %>%
    group_by(name) %>%
    mutate(cafe = row_number()) %>%
    ungroup() %>%
    ggplot(aes(x = a, y = b)) +
    geom_point(aes(color = name)) +
    geom_line(aes(group = cafe))
```

- can do the same for the estimated wait times for each cafe in the morning and afternoon

```{r}
tribble(
    ~ name, ~ morning_wait, ~ afternoon_wait,
    "unpooled", a1, a1 + b1,
    "pooled", a2, a2 + b2
) %>%
    unnest(c(morning_wait, afternoon_wait)) %>%
    group_by(name) %>%
    mutate(cafe = row_number()) %>%
    ungroup() %>%
    ggplot(aes(x = morning_wait, y = afternoon_wait)) +
    geom_point(aes(color = name)) +
    geom_line(aes(group = cafe))
```

## 13.2 Example: Admission decisions and gender

- return to the admissions data and use varying slopes
    * help appreciate how variation in slopes arises
    * and how correlation between intercepts and slopes can provide insight into the underlying process
- from previous models of the `UCBadmit` data:
    * important to have varying means across department otherwise, get wrong inference about gender
    * did not account for variation in how departments treat male and female applications

```{r}
data("UCBadmit")
d <- as_tibble(UCBadmit) %>%
    janitor::clean_names() %>%
    mutate(male = as.numeric(applicant_gender == "male"),
           dept_id = coerce_index(dept))
```

### 13.2.1 Varying intercepts

- first model with only the varying intercepts

$$
A_i \sim \text{Binomial}(n_i, p_i) \\
\text{logit}(p_i) = \alpha_{\text{dept}[i]} + \beta m_i \\
\alpha_\text{dept} \sim \text{Normal}(\alpha, \sigma) \\
\alpha \sim \text{Normal}(0, 10) \\
\beta \sim \text{Normal}(0, 1) \\
\sigma \sim \text{HalfCauchy}(0, 2) \\
$$

```{r}
stash("m13_2", {
    m13_2 <- map2stan(
        alist(
            admit ~ dbinom(applications, p),
            logit(p) <- a_dept[dept_id] + bm*male,
            a_dept[dept_id] ~ dnorm(a, sigma_dept),
            a ~ dnorm(0, 10),
            bm ~ dnorm(0, 1),
            sigma_dept ~ dcauchy(0, 2)
        ),
        data = d,
        warmup = 500, iter = 4500, chains = 3
    )
})

precis(m13_2, depth = 2)
```

- interpretation
    * effect of male is similar that found in Chapter 10 ("Counting and Classification")
        - the intercept is effectively uninteresting, if perhaps slightly negative
    * because we included the global mean $\alpha$ in the prior for the varying intercepts, the `a_dept[i]` values are all deviations from `a`

### 13.2.2 Varying effects of being male

- now we can consider the variation in gender bias among departments
    * use varying slopes
- the data is *imbalanced*
    * the sample sizes vary a lot across departments
    * pooling will have a stronger effect for cases with fewer applicants

$$
A_i \sim \text{Binomial}(n_i, p_i) \\
\text{logit}(p_i) = \alpha_{\text{dept}[i]} + \beta_{\text{dept}[i]} m_i \\

\begin{bmatrix} \alpha_\text{dept} \\ \beta_\text{dept} \end{bmatrix} \sim 
    \text{MVNorm}(\begin{bmatrix} \alpha \\ \beta \end{bmatrix}, \textbf{S}) \\

\alpha \sim \text{Normal}(0, 10) \\
\beta \sim \text{Normal}(0, 1) \\

\textbf{S} = 
    \begin{pmatrix} \sigma_\alpha & 0 \\ 0 & \sigma_\beta \end{pmatrix}
    \textbf{R}
    \begin{pmatrix} \sigma_\alpha & 0 \\ 0 & \sigma_\beta \end{pmatrix} \\

(\sigma_\alpha, \sigma_\beta) \sim \text{HalfCauchy}(0, 2) \\
\textbf{R} \sim \text{LKJcorr}(2)
$$

```{r}
stash("m13_3", {
    m13_3 <- map2stan(
        alist(
            admit ~ dbinom(applications, p),
            logit(p) <- a_dept[dept_id] + bm_dept[dept_id]*male,
            c(a_dept, bm_dept)[dept_id] ~ dmvnorm2(c(a, bm), sigma_dept, Rho),
            a ~ dnorm(0, 10),
            bm ~ dnorm(0, 1),
            sigma_dept ~ dcauchy(0, 2),
            Rho ~ dlkjcorr(2)
        ),
        data = d,
        warmup = 1e3, iter = 5e3, chains = 4
    )
})

precis(m13_3, depth = 2)
```

- focus on what the addition of varying slopes has revealed
    * plot below shows marginal posterior distributions for the varying effects
    * the intercepts are quite varied, but the slopes are all quite close to 0
        - suggests that the departments had different rates of admissions, but none discriminated between male and females
        - one standout is the slope for department 1 which suggests some bias against females
            * department 1 also has the largest intercept, so look into the correlation between slopes and intercepts next

```{r}
plot(precis(m13_3, pars = c("a_dept", "bm_dept"), depth = 2))
```

### 13.2.3 Shrinkage

- following plot shows the posterior distribution for the correlation between slope and intercept
    * negative correlation: the higher the admissions rate, the lower the slope

```{r}
post <- extract.samples(m13_3)
tibble(posterior_rho = post$Rho[, 1, 2]) %>%
    ggplot(aes(x = posterior_rho)) +
    geom_density(size = 1.3, color = dark_grey, fill = grey, alpha = 0.2) +
    scale_x_continuous(expand = expansion(mult = c(0, 0))) +
    scale_y_continuous(expand = expansion(mult = c(0, 0.02))) +
    labs(x = "correlation",
         y = "density",
         title = "Correlation of varying slopes and intercepts")
```

13.2.4 Model comparison

- also fit a model that ignores gender for purposes of comparison

```{r}
stash("m13_4", {
    m13_4 <- map2stan(
        alist(
            admit ~ dbinom(applications, p),
            logit(p) <- a_dept[dept_id],
            a_dept[dept_id] ~ dnorm(a, sigma_dept),
            a ~ dnorm(0, 10),
            sigma_dept ~ dcauchy(0, 2)
        ),
        data = d,
        warmup = 500, iter = 4500, chains = 3
    )
})

compare(m13_2, m13_3, m13_4)
```

- interpretation:
    * the model with no slope for differences in gender `m13_4` performs the same out-of-sample performance as the model with a single slope for a constant effect of gender `m13_2`
    * the model with varying slopes suggests that even though the slope is near zero, it is worth modeling as a separate distribution

## 13.3 Example: Cross-classified chimpanzees with varying slopes

- use chimpanzee data to model multiple varying intercepts and/or slopes
    * varying intercepts for `actor` and `block`
    * varying slopes for prosocial option and the interaction between prosocial and the presence of another chimpanzee
- *non-centered parameterization* (see later for explanation and example)
    * there are always several ways to formulate a model that are mathematically equivalent
    * however, they can result in different sampling results, so the parameterization is part of the model
-  cross-classified varying slopes model
    * use multiple linear models to compartmentalize sub-models for the intercepts and each slope
    * $\mathcal{A}_i$, $\mathcal{B}_{P,i}$, and $\mathcal{B}_{PC,i}$ are the sub-models

$$
L_i \sim \text{Binomial}(1, p_i) \\
\text{logit}(p_i) = \mathcal{A}_i + (\mathcal{B}_{p,i} + \mathcal{B}_{PC,i} C_i) P_i \\
\mathcal{A}_i = \alpha + \alpha_{\text{actor}[i]} + \alpha_{\text{block}[i]} \\
\mathcal{B}_{P,i} = \beta_P + \beta_{P,\text{actor}[i]} + \beta_{P,\text{block}[i]} \\
\mathcal{B}_{PC,i} = \beta_P + \beta_{PC,\text{actor}[i]} + \beta_{PC,\text{block}[i]} \\
$$

- below is the formulation for the multivariate priors
    * one multivariate Gaussian per cluster of the data (`actor` and `block`)
    * for this model, each is 3D, one for each variable in the model
        - this can be adjusted to have different varying effects in different cluster types
    * these priors state that the actors and blocks come from different statistical populations
        - within each, three features for each actor or block are related through a covariance matrix for the population ($\textbf{S}$)
        - the mean for each prior is 0 because there is an average value in the linear models already ($\alpha$, $\beta_P$, and $\beta_{PC,i}$)

$$
\begin{bmatrix} 
    \alpha_\text{actor} \\ \beta_{P,\text{actor}} \\ \beta_{PC,\text{actor}}
\end{bmatrix}
\sim \text{MVNormal}
\begin{pmatrix}
    \begin{bmatrix}
        0 \\ 0 \\ 0
    \end{bmatrix}, \textbf{S}_\text{actor}
\end{pmatrix} \\

\begin{bmatrix} 
    \alpha_\text{block} \\ \beta_{P,\text{block}} \\ \beta_{PC,\text{block}}
\end{bmatrix}
\sim \text{MVNormal}
\begin{pmatrix}
    \begin{bmatrix}
        0 \\ 0 \\ 0
    \end{bmatrix}, \textbf{S}_\text{block}
\end{pmatrix}
$$

```{r}
data("chimpanzees")
d <- as_tibble(chimpanzees) %>%
    select(-recipient) %>%
    rename(block_id = block)

stash("m13_6", {
    m13_6 <- map2stan(
        alist(
            pulled_left ~ dbinom(1, p),
            logit(p) <- A + (BP + BPC*condition) * prosoc_left,
            A <- a + a_actor[actor] + a_block[block_id],
            BP <- bp + bp_actor[actor] + bp_block[block_id],
            BPC <- bpc + bpc_actor[actor] + bpc_block[block_id],
            
            c(a_actor, bp_actor, bpc_actor)[actor] ~ dmvnorm2(0, sigma_actor, Rho_actor),
            c(a_block, bp_block, bpc_block)[block_id] ~ dmvnorm2(0, sigma_block, Rho_block),
            
            c(a, bp, bpc) ~ dnorm(0, 1),
            sigma_actor ~ dcauchy(0, 2),
            sigma_block ~ dcauchy(0, 2),
            Rho_actor ~ dlkjcorr(4),
            Rho_block ~ dlkjcorr(4)
        ),
        data = d,
        iter = 5e3, warmup = 1e3, chains = 3
    )
})

precis(m13_6, depth = 1)
```

- there was an issue with the HMC sampling 
    * can often just do more sampling to get over it, but other times the chains may not converge
    * this is where *non-centered parameterization* can help

> In map2stan(alist(pulled_left ~ dbinom(1, p), logit(p) <- A + (BP + :
> There were 559 divergent iterations during sampling.
> Check the chains (trace plots, n_eff, Rhat) carefully to ensure they are valid.

- use *non-centered parameterization* to help with potentially diverging chains
    * use an alternative parameterization of the model using `dmvnormNC()`
    * mathematically equivalent to the first

```{r}
stash("m13_6nc", {
    m13_6nc <- map2stan(
        alist(
            pulled_left ~ dbinom(1, p),
            logit(p) <- A + (BP + BPC*condition) * prosoc_left,
            A <- a + a_actor[actor] + a_block[block_id],
            BP <- bp + bp_actor[actor] + bp_block[block_id],
            BPC <- bpc + bpc_actor[actor] + bpc_block[block_id],
            
            c(a_actor, bp_actor, bpc_actor)[actor] ~ dmvnormNC(sigma_actor, Rho_actor),
            c(a_block, bp_block, bpc_block)[block_id] ~ dmvnormNC(sigma_block, Rho_block),
            
            c(a, bp, bpc) ~ dnorm(0, 1),
            sigma_actor ~ dcauchy(0, 2),
            sigma_block ~ dcauchy(0, 2),
            Rho_actor ~ dlkjcorr(4),
            Rho_block ~ dlkjcorr(4)
        ),
        data = d,
        iter = 5e3, warmup = 1e3, chains = 3
    )
})

precis(m13_6nc, depth = 1)
```

- the non-centered parameterization helped by sampling faster and more effectively

```{r}
get_neff <- function(mdl, depth) {
    x <- precis(mdl, depth = depth)
    neff_idx <- which(x@names == "n_eff")
    x@.Data[neff_idx]
}

tibble(m13_6 = get_neff(m13_6, depth = 2),
       m13_6nc = get_neff(m13_6nc, depth = 2)) %>%
    pivot_longer(tidyselect::everything()) %>%
    unnest(value) %>%
    ggplot(aes(name, value)) +
    geom_boxplot(outlier.shape = NA) +
    ggbeeswarm::geom_quasirandom(color = grey, size = 2, alpha = 0.8) +
    labs(x = "model",
         y = "effective samples")
```

- can see the number of effective parameters is much smaller than the real number

```{r}
WAIC(m13_6nc)
```

- the standard deviation parameters of the random effects provides a measure of how much regularization was applied
    * the first index for each sigma is the varying intercept standard deviation while the other two are the slopes
    * the values are pretty small suggesting there was a good amount of shrinkage

```{r}
precis(m13_6nc, depth = 2, pars = c("sigma_actor", "sigma_block"))
```

- compare the varying slopes model to the simpler varying intercepts model from the previous chapter
    * the results indicate that there isn't too much difference between the two models
    * meaning there isn't much difference in the slopes between `actor` nor `block`

```{r}
stash("m12_5", {
    m12_5 <- map2stan(
        alist(
            pulled_left ~ dbinom(1, p),
            logit(p) <- a + a_actor[actor] + a_block[block_id] + (bp + bpc*condition)*prosoc_left,
            a_actor[actor] ~ dnorm(0, sigma_actor),
            a_block[block_id] ~ dnorm(0, sigma_block),
            a ~ dnorm(0, 10),
            bp ~ dnorm(0, 10),
            bpc ~ dnorm(0, 10),
            sigma_actor ~ dcauchy(0, 1),
            sigma_block ~ dcauchy(0, 1)
        ),
        data = d,
        warmup = 1e3,
        iter = 6e3,
        chains = 4
    )
})

compare(m13_6nc, m12_5)
```

## 13.4 Continuous categories and the Gaussian process

- so far, all varying intercepts and slopes were defined over discrete, unordered categories
    * now learn how to use continuous dimensions of variation
        - e.g.: age, income, social standing
- *Gaussian process regression*: method for applying a varying effect to continuous categories
    * estimates a unique intercept/slope for any value in the variable and applies shrinkage to these values
    * simple outline of the process:
        - calculate differences between all data points in the category
        - the model estimates a function for the covariance between pairs of cases at each distance
        - the coviariance function is the generalization of the varying effects approach to continuous categories

### 13.4.1 Example: Spatial autocorrelation in Oceanic tools

- in previous modeling of the Oceanic societies data, used a binary contact predictor
    * want to make a model that keeps this as a continuous variable
    * many reasons why islands near each other would have similar tools
- the process:
    * define a distance matrix among the societies
    * then estimate how similarity in tool counts depends on geographic distance

```{r}
data("islandsDistMatrix")
Dmat <- islandsDistMatrix
colnames(Dmat) <- c("Ml","Tiv","SC","Ya","Fi","Tr","Ch","Mn","To","Ha")
round(Dmat, 1)
```

- the likelihood and linear model for this model look the same as before:
    * Poisson likelihood with a varying intercept linear model with a log link function
    * the $\gamma_{\text{society}}$ is the varying intercept
    * regular coefficient for log population
        - determine if accounting for spatial similarity will wash out the association between log population and total number of tools

$$
T_i \sim \text{Poisson}(\lambda_i) \\
\log \lambda_i = \alpha + \gamma_{\text{society}[i]} + \beta_P \log P_i
$$

- add in a multivariate prior for the intercepts for the Gaussian process
    * first is the 10-dimensional Gaussian prior for the intercepts
    * $\textbf{K}$ is the covariance matrix between any pairs of societies $i$ and $j$
        - three new parameters: $\eta$, $\rho$, and $\sigma$
        - the Gaussian shape comes from $\exp(-\rho^2 D_{ij}^2)$ where $D_{ij}$ is the distance between societies $i$ and $j$
            * says that the covariance between two societies declines exponentially with the squared distance
            * $\rho$ determines the rate of decline (large = declines rapidly with distance)
            * the distance need not be squared, but usually is because it is often a more realistic model and fits more easily
        - $\eta^2$ is the maximum covariance between two societies $i$ and $j$
        - $\delta_{ij}\sigma^2$ provides for extra covariance beyond $\eta^2$ when $i=j$
            * the function $\delta_{ij}$ is 1 when $i=j$, else 0
            * this only matters if there is more than one data point per group (which there isn't in the Oceanic example)
            * therefore, $sigma$ describes how the observations for a single category covary

$$
\gamma \sim \text{MVNormal}([0, ..., 0], \textbf{K}) \\
\textbf{K}_{ij} = \eta^2 \exp(-\rho^2 D_{ij}^2) + \delta_{ij}\sigma^2
$$

- the full model
    * create priors for $\eta^2$ and $\rho^2$ because is easier to fit
    * set $\sigma$ as a small constant because it does not get used in this model (see above)

$$
T_i \sim \text{Poisson}(\lambda_i) \\
\log \lambda_i = \alpha + \gamma_{\text{society}[i]} + \beta_P \log P_i \\
\gamma \sim \text{MVNormal}([0, ..., 0], \textbf{K}) \\
\textbf{K}_{ij} = \eta^2 \exp(-\rho^2 D_{ij}^2) + \delta_{ij}(0.01) \\
\alpha \sim \text{Normal}(0, 10) \\
\beta_P \sim \text{Normal}(0, 1) \\
\eta^2 \sim \text{HalfCauchy}(0, 1) \\
\rho^2 \sim \text{HalfCauchy}(0, 1)
$$

- fit using `map2stan()`
    * use `GPL2()` in order to use a squared distance Gaussian process prior

```{r}
data("Kline2")
d <- as_tibble(Kline2) %>%
    mutate(society = row_number())

stash("m13_7", {
    m13_7 <- map2stan(
        alist(
            total_tools ~ dpois(lambda),
            log(lambda) <- a + g[society] + bp*logpop,
            g[society] ~ GPL2(Dmat, etasq, rhosq, 0.01),
            a ~ dnorm(0, 10),
            bp ~ dnorm(0, 1),
            etasq ~ dcauchy(0, 1),
            rhosq ~ dcauchy(0, 1)
        ),
        data = list(total_tools = d$total_tools,
                    logpop = d$logpop,
                    society = d$society,
                    Dmat = islandsDistMatrix),
        warmup = 2e3, iter = 1e4, chains = 4
    )
})

plot(m13_7)
precis(m13_7, depth=2)
```

- interpretation:
    * the coefficient for log population `bp` is the same as before adding in the Gaussian process for varying intercepts
        - the association between tool counts and population cannot be explained by spatial correlations
- plot posterior of covariance functions using `rhosp` and `etasq` samples

```{r}
post <- extract.samples(m13_7)

median_covar_df <- tibble(etasq = median(post$etasq),
                          rhosq = median(post$rhosq),
                          distance = seq(1, 10, length.out = 100)) %>%
    mutate(density = etasq * exp(-rhosq * distance^2),
           color = "median",
           group = "median")

sample_covar_df <- tibble(etasq = post$etasq[1:100],
                          rhosq = post$rhosq[1:100]) %>%
    mutate(group = as.character(row_number()),
           distance = list(rep(seq(1, 10, length.out = 100), n()))) %>%
    unnest(distance) %>%
    mutate(density = etasq * exp(-rhosq * distance^2),
           color = "posterior samples")

bind_rows(median_covar_df, sample_covar_df) %>%
    ggplot(aes(distance, density)) +
    geom_line(aes(group = group, alpha = color, size = color, color = color)) +
    scale_alpha_manual(values = c(0.8, 0.2)) +
    scale_size_manual(values = c(2, 0.4)) +
    scale_color_manual(values = c(blue, "grey20")) +
    scale_y_continuous(limits = c(0, 1),
                       expand = c(0, 0)) +
    theme(legend.position = c(0.85, 0.7),
          legend.title = element_blank()) +
    labs(x = "distance (thousand km)",
         y = "covariance",
         title = "Posterior distribution of covariance functions")
```

- consider the covariations among societies that are implied by the posterior median
