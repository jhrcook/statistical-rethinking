---
title: "Chapter 13. Adventures in Covariance"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      comment = "#>",
                      cache = TRUE,
                      dpi = 300)

library(glue)
library(mustashe)
library(magrittr)
library(broom)
library(patchwork)
library(MASS)
library(rethinking)
library(tidyverse)
library(conflicted)


conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("rename", "dplyr")

# To be able to use `map2stan()`:
conflict_prefer("collapse", "dplyr")
conflict_prefer("extract", "tidyr")
conflict_prefer("rstudent", "rethinking")
conflict_prefer("lag", "dplyr")
conflict_prefer("map", "purrr")
conflict_prefer("Position", "ggplot2")

# To be able to use 'MASS':
conflict_prefer("area", "patchwork")

theme_set(theme_minimal())
source_scripts()
set.seed(0)

# For 'rethinking'.
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

purple <- "#8067bf"
blue <- "#5896d1"
red <- "#d15858"
light_grey <- "grey80"
grey <- "grey50"
dark_grey <- "grey25"

# To shut-up `summarise()` in dplyr 1.0.0
options(dplyr.summarise.inform = FALSE)
```

- this chapter will show how to specify *varying slopes* in combination with varying intercepts
    * enables pooling to improve estimates of how different units respond to or are influenced by predictor variables
    * also improves the estimates of intercepts by "borrowing information across parameter types"
    * "varying slope models are massive interaction machines"

## 13.1 Varying slopes by construction

- pool information across intercepts and slopes by modeling the joint population of intercepts and slopes
    * modeling their covariance
    * assigning a 2D Gaussian distribution to both the intercepts (first dimension) and the slopes (second dimension)
- the variance-covariance matrix for a fit model describes how each parameters posterior probability is associated with one another
    * varying intercepts have variation, varying slopes have variation, and intercepts and slopes covary
- use example of visiting coffee shops:
    * visit different cafes, order a coffee, and record the wait time
        - previously, used varying intercepts, one for each cafe
    * also record the time of day
        - the average wait time is longer in the mornings than afternoons because they are busier in the mornings
    * different cafes vary in their average wait times and their differences between morning and afternoon
        - the differences in wait time by time of day are the slopes
    * cafes covary in their intercepts and slopes
        - because the popular cafes have much longer wait times in the morning leading to large differences between morning and afternoon

$$
\mu_i = \alpha_{\text{cafe}[i]} + \beta_{\text{cafe}[i]} A_i
$$

## 13.1.1 Simulate the population

- define the population of cafes
    * define the average wait time in the morning and afternoon
    * define the correlation between them

```{r}
a <- 3.5        # average morning wait time
b <- -1         # average difference afternoon wait time
sigma_a <- 1    # standard deviation in intercepts
sigma_b <- 0.5  # standard deviation in slopes
rho <- -0.7     # correlation between intercepts and slopes
```

- use these values to simulate a sample of cafes
    * define the multivariate Gaussian with a vecotr of means and a 2x2 matrix of variances and covariances

```{r}
Mu <- c(a, b)  # vector of two means
```

- the matrix of variances and covariances is arranged as follows

$$
\begin{pmatrix}
\sigma_\alpha^2 & \sigma_\alpha \sigma_\beta \rho \\
\sigma_\alpha \sigma_\beta \rho & \sigma_\beta^2
\end{pmatrix}
$$

- can construct the matrix explicitly

```{r}
cov_ab <- sigma_a * sigma_b * rho
Sigma <- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), nrow = 2)
Sigma
```

- another way to build the variance-covariance matrix using matrix multiplication
    * this is likely a better approach with larger models

```{r}
sigmas <- c(sigma_a, sigma_b)
Rho <- matrix(c(1, rho, rho, 1), nrow = 2)
Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)
Sigma
```

- now can simulate 20 cafes, each with their own intercept and slope

```{r}
N_cafes <- 20
```

- simulate from a multivariate Gaussian using `mvnorm()` from the 'MASS' package
    * returns a matrix of $\text{cafe} \times (\text{intercept}, \text{slope})$

```{r}
library(MASS)

set.seed(5)
vary_effects <- mvrnorm(n = N_cafes, mu = Mu, Sigma = Sigma)
head(vary_effects)
```

```{r}
# Split into separate vectors for ease of use later.
a_cafe <- vary_effects[, 1]
b_cafe <- vary_effects[, 2]
cor(a_cafe, b_cafe)
```

- plot of the varying effects

```{r}
as.data.frame(vary_effects) %>%
    as_tibble() %>%
    set_names(c("intercept", "slope")) %>%
    ggplot(aes(x = intercept, y = slope)) +
    geom_point()
```

### 13.1.2 Simulate observations

- simulate the visits to each cafe
    * 10 visits to each cafe, 5 in the morning and 5 in the afternoon

```{r}
N_visits <- 10
afternoon <- rep(0:1, N_visits * N_cafes / 2)
cafe_id <- rep(1:N_cafes, each = N_visits)

# Get the average wait time for each cafe in the morning and afternoon.
mu <- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon

sigma <- 0.5  # Standard deviation within cafes

# Sample wait times with each cafes unique average wait time per time of day.
wait_times <- rnorm(N_visits*N_cafes, mu, sigma)

d <- tibble(cafe = cafe_id, afternoon, wait = wait_times)
d
```

```{r}
d %>%
    ggplot(aes(x = wait, y = afternoon, color = factor(cafe))) +
    geom_jitter(width = 0, height = 0.2) +
    scale_color_manual(values = randomcoloR::distinctColorPalette(N_cafes),
                       guide = FALSE)
```

```{r}
d %>%
    ggplot(aes(x = wait, y = factor(cafe), color = factor(afternoon))) +
    geom_point() +
    scale_color_manual(values = c(blue, red))
```

### 13.1.3 The varying slopes model

- model with varying intercepts and slopes (explination follows)

$$
W_i \sim \text{Normal}(\mu_i, \sigma) \\

\mu_i = \alpha_{\text{cafe}[i]} + \beta_{\text{cafe}[i]} A_i \\

\begin{bmatrix}
    \alpha_\text{cafe} \\
    \beta_\text{cafe}
\end{bmatrix} \sim \text{MVNormal}(
    \begin{bmatrix} \alpha \\ \beta \end{bmatrix}, \textbf{S}
) \\

\textbf{S} = 
\begin{pmatrix} 
    \sigma_\alpha & 0 \\ 0 &\sigma_\beta
\end{pmatrix}
\textbf{R}
\begin{pmatrix} 
    \sigma_\alpha & 0 \\ 0 &\sigma_\beta
\end{pmatrix} \\

\alpha \sim \text{Normal}(0,10) \\
\beta \sim \text{Normal}(0,10) \\
\sigma \sim \text{HalfCauchy}(0, 1) \\
\sigma_\alpha \sim \text{HalfCauchy}(0, 1) \\
\sigma_\beta \sim \text{HalfCauchy}(0, 1) \\
\textbf{R} \sim \text{LKJcorr}(2)
$$

- the third like defines the population of varying intercepts and slopes        
    * each cafe has an intercept and slope with a prior distribution defined by the 2D Gaussian distribution with means $\alpha$ and $\beta$ and covariance matrix $\text{S}$

$$
\begin{bmatrix}
    \alpha_\text{cafe} \\
    \beta_\text{cafe}
\end{bmatrix} \sim \text{MVNormal}(
    \begin{bmatrix} \alpha \\ \beta \end{bmatrix}, \textbf{S}
)
$$

- the next line defines the variance-covariance matrix $\textbf{S}$
    * factoring it into simple standard deviations $\sigma_\alpha$ and $\sigma_\beta$ and a correlation matrix $\textbf{R}$
    * there are other ways to do this, but this formulation helps understand the inferred structure of the varying effects

$$
\textbf{S} = 
\begin{pmatrix} 
    \sigma_\alpha & 0 \\ 0 &\sigma_\beta
\end{pmatrix}
\textbf{R}
\begin{pmatrix} 
    \sigma_\alpha & 0 \\ 0 &\sigma_\beta
\end{pmatrix}
$$

- the correlation matrix has a prior defined as $\textbf{R} \sim \text{LKJcorr}(2)$
    * the correlation matrix will have the structure: $\begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}$ where $\rho$ is the correlation between the intercepts and slopes
    * with additional varying slopes, there are more correlation parameters, but the $\text{LKJcorr}$ prior will still work
    * the $\text{LKJcorr}(2)$ prior defines a weakly informative prior on $\rho$ that is skeptical of extreeme correlations near -1 and 1
    * it has a single parameter $\eta$ that controls how "skeptical" the prior is of large corrleations
        - if $\eta=1$, the prior is flat from -1 to 1
        - a large value of $\eta$ the mass of the distribution moves towards 0

```{r}
tibble(eta = c(1, 2, 4)) %>%
    mutate(value = map(eta, ~ rlkjcorr(1e5, K = 2, eta = .x)[, 1, 2])) %>%
    unnest(value) %>%
    ggplot(aes(x = value)) +
    geom_density(aes(color = factor(eta), fill = factor(eta)), 
                 size = 1.3, alpha = 0.1) +
    scale_color_brewer(palette = "Set2") +
    scale_fill_brewer(palette = "Set2") +
    labs(x = "correlation",
         y = "density",
         title = "Distributions from LKJcorr for different scale values",
         color = "eta", fill = "eta")
```

- now can fit the model

```{r}
stash("m13_1", {
    m13_1 <- map2stan(
        alist(
            wait ~ dnorm(mu, sigma),
            mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,
            c(a_cafe, b_cafe)[cafe] ~ dmvnorm2(c(a, b), sigma_cafe, Rho),
            a ~ dnorm(0, 10),
            b ~ dnorm(0, 10),
            sigma_cafe ~ dcauchy(0, 2),
            sigma ~ dcauchy(0, 2),
            Rho ~ dlkjcorr(2)
        ),
        data = d,
        iter = 5e3, warmup = 2e3, chains = 2
    )
})

precis(m13_1, depth = 1)
```

- inspection of the posteior distribution of varying effects
    * start with the posterior correlation between intercepts and slopes
        - the posterior distribution of the correlation between varying effects is decidedly negative

```{r}
post <- extract.samples(m13_1)

tribble(
    ~name, ~value,
    "posterior", post$Rho[, 1, 2],
    "prior", rlkjcorr(1e5, K = 2, eta = 2)[, 1, 2]
) %>%
    unnest(value) %>%
    ggplot(aes(x = value, color = name, fill = name)) +
    geom_density(size = 1.3, alpha = 0.2) +
    scale_color_manual(values = c(blue, grey)) +
    scale_fill_manual(values = c(blue, grey)) +
    theme(legend.title = element_blank(),
          legend.position = c(0.85, 0.5)) +
    labs(x = "correlation",
         y = "probability density",
         title = "Varying effect correlation posterior distribution")
```

- consider the shrinkage
    * the inferred correlation between varying effects pooled information across them
    * and the inferred variation within each varying effect was pooled
    * together the variances and correlation define a multivariate Gaussian prior for the varying effects
    * this prior regularizes the intercepts and slopes
- plot the posterior mean varying effects
    * compare them to the raw, unpooled estimates
    * also plot the inferred prior for the population of intercepts and slopes

> There is something wrong with the following 2 plots, but I cannot figure out what went wrong.

```{r}
# Raw, unpooled estimates for alpha and beta.
a1 <- map_dbl(1:N_cafes, function(i) {
    mean(d$wait[d$cafe == i & d$afternoon == 0])
})

b1 <- map_dbl(1:N_cafes, function(i) {
    mean(d$wait[d$cafe == i & d$afternoon == 1])
})
b1 <- b1 - a1

# Extract posterior means of partially pooled estimates.
post <- extract.samples(m13_1)
a2 <- apply(post$a_cafe, 2, mean)
b2 <- apply(post$b_cafe, 2, mean)

tribble(
    ~ name, ~ a, ~ b,
    "unpooled", a1, b1,
    "pooled", a2, b2
) %>%
    unnest(c(a, b)) %>%
    group_by(name) %>%
    mutate(cafe = row_number()) %>%
    ungroup() %>%
    ggplot(aes(x = a, y = b)) +
    geom_point(aes(color = name)) +
    geom_line(aes(group = cafe))
```

- can do the same for the estimated wait times for each cafe in the morning and afternoon

```{r}
tribble(
    ~ name, ~ morning_wait, ~ afternoon_wait,
    "unpooled", a1, a1 + b1,
    "pooled", a2, a2 + b2
) %>%
    unnest(c(morning_wait, afternoon_wait)) %>%
    group_by(name) %>%
    mutate(cafe = row_number()) %>%
    ungroup() %>%
    ggplot(aes(x = morning_wait, y = afternoon_wait)) +
    geom_point(aes(color = name)) +
    geom_line(aes(group = cafe))
```

## 13.2 Example: Admission decisions and gender

- return to the admissions data and use varying slopes
    * help appreciate how variation in slopes arises
    * and how correlation between intercepts and slopes can provide insight into the underlying process
- from previous models of the `UCBadmit` data:
    * important to have varying means across department otherwise, get wrong inference about gender
    * did not account for variation in how departments treat male and female applications

```{r}
data("UCBadmit")
d <- as_tibble(UCBadmit) %>%
    janitor::clean_names() %>%
    mutate(male = as.numeric(applicant_gender == "male"),
           dept_id = coerce_index(dept))
```

### 13.2.1 Varying intercepts

- first model with only the varying intercepts

$$
A_i \sim \text{Binomial}(n_i, p_i) \\
\text{logit}(p_i) = \alpha_{\text{dept}[i]} + \beta m_i \\
\alpha_\text{dept} \sim \text{Normal}(\alpha, \sigma) \\
\alpha \sim \text{Normal}(0, 10) \\
\beta \sim \text{Normal}(0, 1) \\
\sigma \sim \text{HalfCauchy}(0, 2) \\
$$

```{r}
stash("m13_2", {
    m13_2 <- map2stan(
        alist(
            admit ~ dbinom(applications, p),
            logit(p) <- a_dept[dept_id] + bm*male,
            a_dept[dept_id] ~ dnorm(a, sigma_dept),
            a ~ dnorm(0, 10),
            bm ~ dnorm(0, 1),
            sigma_dept ~ dcauchy(0, 2)
        ),
        data = d,
        warmup = 500, iter = 4500, chains = 3
    )
})

precis(m13_2, depth = 2)
```

- interpretation
    * effect of male is similar that found in Chapter 10 ("Counting and Classification")
        - the intercept is effectively uninteresting, if perhaps slightly negative
    * because we included the global mean $\alpha$ in the prior for the varying intercepts, the `a_dept[i]` values are all deviations from `a`

### 13.2.2 Varying effects of being male

- now we can consider the variation in gender bias among departments
    * use varying slopes
- the data is *imbalanced*
    * the sample sizes vary a lot across departments
    * pooling will have a stronger effect for cases with fewer applicants

$$
A_i \sim \text{Binomial}(n_i, p_i) \\
\text{logit}(p_i) = \alpha_{\text{dept}[i]} + \beta_{\text{dept}[i]} m_i \\

\begin{bmatrix} \alpha_\text{dept} \\ \beta_\text{dept} \end{bmatrix} \sim 
    \text{MVNorm}(\begin{bmatrix} \alpha \\ \beta \end{bmatrix}, \textbf{S}) \\

\alpha \sim \text{Normal}(0, 10) \\
\beta \sim \text{Normal}(0, 1) \\

\textbf{S} = 
    \begin{pmatrix} \sigma_\alpha & 0 \\ 0 & \sigma_\beta \end{pmatrix}
    \textbf{R}
    \begin{pmatrix} \sigma_\alpha & 0 \\ 0 & \sigma_\beta \end{pmatrix} \\

(\sigma_\alpha, \sigma_\beta) \sim \text{HalfCauchy}(0, 2) \\
\textbf{R} \sim \text{LKJcorr}(2)
$$

```{r}
stash("m13_3", {
    m13_3 <- map2stan(
        alist(
            admit ~ dbinom(applications, p),
            logit(p) <- a_dept[dept_id] + bm_dept[dept_id]*male,
            c(a_dept, bm_dept)[dept_id] ~ dmvnorm2(c(a, bm), sigma_dept, Rho),
            a ~ dnorm(0, 10),
            bm ~ dnorm(0, 1),
            sigma_dept ~ dcauchy(0, 2),
            Rho ~ dlkjcorr(2)
        ),
        data = d,
        warmup = 1e3, iter = 5e3, chains = 4
    )
})

precis(m13_3, depth = 2)
```

- focus on what the addition of varying slopes has revealed
    * plot below shows marginal posterior distributions for the varying effects
    * the intercepts are quite varied, but the slopes are all quite close to 0
        - suggests that the departments had different rates of admissions, but none discriminated between male and females
        - one standout is the slope for deptartment 1 which suggests some bias against females
            * department 1 also has the largest intercept, so look into the correlation between slopes and intercepts next

```{r}
plot(precis(m13_3, pars = c("a_dept", "bm_dept"), depth = 2))
```

### 13.2.3 Shrinkage

- following plot shows the posterior distribution for the correlation between slope and intercept
    * negative correlation: the higher the admissions rate, the lower the slope

```{r}
post <- extract.samples(m13_3)
tibble(posterior_rho = post$Rho[, 1, 2]) %>%
    ggplot(aes(x = posterior_rho)) +
    geom_density(size = 1.3, color = dark_grey, fill = grey, alpha = 0.2) +
    scale_x_continuous(expand = expansion(mult = c(0, 0))) +
    scale_y_continuous(expand = expansion(mult = c(0, 0.02))) +
    labs(x = "correlation",
         y = "density",
         title = "Correlation of varying slopes and intercepts")
```

13.2.4 Model comparison

- also fit a model that ignores gender for purposes of comparison

```{r}
stash("m13_4", {
    m13_4 <- map2stan(
        alist(
            admit ~ dbinom(applications, p),
            logit(p) <- a_dept[dept_id],
            a_dept[dept_id] ~ dnorm(a, sigma_dept),
            a ~ dnorm(0, 10),
            sigma_dept ~ dcauchy(0, 2)
        ),
        data = d,
        warmup = 500, iter = 4500, chains = 3
    )
})

compare(m13_2, m13_3, m13_4)
```

- interpretation:
    * the model with no slope for differences in gender `m13_4` performs the same out-of-sample performance as the model with a single slope for a constant effect of gender `m13_2`
    * the model with varying slopes suggests that even though the slope is near zero, it is worth modeling as a separate distribution

## 13.3 Example: Cross-classified chimpanzees with varying slopes



