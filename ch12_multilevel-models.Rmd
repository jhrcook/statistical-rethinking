---
title: "Chapter 12. Multilevel Models"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = TRUE)

library(glue)
library(mustashe)
library(broom)
library(patchwork)
library(MASS)
library(rethinking)
library(tidyverse)
library(conflicted)


conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("rename", "dplyr")

# To be able to use `map2stan()`:
conflict_prefer("collapse", "dplyr")
conflict_prefer("extract", "tidyr")
conflict_prefer("rstudent", "rethinking")
conflict_prefer("lag", "dplyr")
conflict_prefer("map", "purrr")
conflict_prefer("Position", "ggplot2")

# To be able to use 'MASS':
conflict_prefer("area", "patchwork")

theme_set(theme_minimal())
source_scripts()
set.seed(0)

# For 'rethinking'.
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

purple <- "#8067bf"
blue <- "#5896d1"
light_grey <- "grey80"
grey <- "grey50"
dark_grey <- "grey25"
```

- multi-level models remember features of each cluster in the data as they learn about all of the clusters
    * depending on the variation across clusters, the model pools information across clusters
    * *the pooling improves estimates about each cluster*
- benefits of the multilevel approach:
    1. improved estimates for repeat sampling
    2. improved estimates for imbalance in sampling
    3. estimates of variation
    4. avoid averaging and retain variation
- multilevel regression should be the default approach
- this chapter starts with the foundations and the following two are more advanced types of multilevel models

## 12.1 Example: Multilivel tadpoles

- example: Reed frog tadpole mortality
    * `surv`: number or survivors
    * `count`: initial number

```{r}
data("reedfrogs")
d <- as_tibble(reedfrogs)
skimr::skim(d)
```

- there is a lot of variation in the data
    * some from experimental treatment, other sources do exist
    * each row is a fish tank that is the experimental environment
    * each tank is a cluster variable and there are repeated measures from each
    * each tank may have a different baseline level of survival, but don't want to treat them as completely unrelated
        - a dummy variable for each tank would be the wrong solution
- *varying intercepts model*: a multilevel model that estimates an intercept for each tank and the variation among tanks
    * for each cluster in the data, use a unique intercept parameter, adaptively learning the prior common to all of the intercepts
    * what is learned about each cluster informs all the other clusters
- model for predicting tadpole mortality in each tank (nothing new)

$$
s_i \sim \text{Binomial}(n_i, p_i) \\
\text{logit}(p_i) = \alpha_{\text{tank}[i]} \\
\alpha_{\text{tank}} \sim \text{Normal}(0, 5) \\
$$

```{r}
d$tank <- 1:nrow(d)

stash("m12_1", {
    m12_1 <- map2stan(
        alist(
            surv ~ dbinom(density, p),
            logit(p) <- a_tank[tank],
            a_tank[tank] ~ dnorm(0, 5)
        ),
        data = d
    )
})


print(m12_1)
precis(m12_1, depth = 2)
```

- can get expected mortality for each tank by taking the logistic of the coefficients

```{r}
logistic(coef(m12_1)) %>%
    enframe() %>%
    mutate(name = str_remove_all(name, "a_tank\\[|\\]"),
           name = as.numeric(name)) %>%
    ggplot(aes(x = name, y = value)) +
    geom_col() +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = expansion(mult = c(0, 0.02))) +
    labs(x = "tank",
         y = "estimated probability survival",
         title = "Single-level categorical model estimates of tadpole survival")
```

- fit a multilevel model by adding a prior for the `a_tank` parameters as a function of its own parameters
    * now the priors have prior distributions, creating two *levels* of priors

$$
s_i \sim \text{Binomial}(n_i, p_i) \\
\text{logit}(p_i) = \alpha_{\text{tank}[i]} \\
\alpha_{\text{tank}} \sim \text{Normal}(\alpha, \sigma) \\
\alpha \sim \text{Normal}(0, 1) \\
\sigma \sim \text{HalfCauchy}(0, 1)
$$

```{r}
stash("m12_2", {
    m12_2 <- map2stan(
        alist(
            surv ~ dbinom(density, p),
            logit(p) <- a_tank[tank],
            a_tank[tank] ~ dnorm(a, sigma),
            a ~ dnorm(0, 1),
            sigma ~ dcauchy(0, 1)
        ),
        data = d,
        iter = 4000,
        chains = 4,
        cores = 1
    )
})


print(m12_2)
precis(m12_2, depth = 2)
```
- interpretation:
    * $\alpha$: one overall sample intercept
    * $\sigma$: variance among tanks
    * 48 per-tank intercepts

```{r}
compare(m12_1, m12_2)
```

- from the comparison, see that the multilevel model only has ~38 effective parameters
    * 12 fewer than the single-level model because the prior assigned to each intercept shrinks them all towards the mean $\alpha$
        - *$\alpha$ is acting like a regularizing prior, but it has been learned from the data*
- plot and compare the posterior medians from both models

```{r}
post <- extract.samples(m12_2)

d %>%
    mutate(propsurv_estimate = logistic(apply(post$a_tank, 2, median)),
           pop_size = case_when(
               density == 10 ~ "small tank",
               density == 25 ~ "medium tank",
               density == 35 ~ "large tank"
           ),
           pop_size = fct_reorder(pop_size, density)) %>%
    ggplot(aes(tank)) +
    facet_wrap(~ pop_size, nrow = 1, scales = "free_x") +
    geom_hline(yintercept = logistic(median(post$a)), 
               lty = 2, color = dark_grey) +
    geom_linerange(aes(x = tank, ymin = propsurv, ymax = propsurv_estimate), 
                   color = light_grey, size = 1) +
    geom_point(aes(y = propsurv), 
               color = grey, size = 1) +
    geom_point(aes(y = propsurv_estimate), 
               color = purple, size = 1) +
    labs(x = "tank",
         y = "proportion surivival",
         title = "Propotion of survival among tadpoles from different tanks.")
```

- comments on above plot:
    * note that all of the purple points $\alpha_\text{tank}$ are skewed towards to the dashed line $\alpha$
        - this is often called *shrinkage* and comes from regularization
    * note that the smaller tanks have shifted more than in the larger tanks
        - there are fewer starting tadpoles, so the shrinkage has a stronger effect
    * the shift of the purple points is large the further the empirical value (grey points) are from the dashed line $\alpha$
- sample from the posterior distributions:
    * first plot 100 Gaussian distributions from samples of the posteriors for $\alpha$ and $\sigma$
    * then sample 8000 new log-odds of survival for individual tanks

```{r}
x <- seq(-3, 5, length.out = 300)
log_odds_gaussian_samples <- map_dfr(1:100, function(i) {
    tibble(i, x, prob = dnorm(x, post$a[i], post$sigma[i]))
})

p1 <- log_odds_gaussian_samples %>%
    ggplot(aes(x, prob, group = factor(i))) +
    geom_line(alpha = 0.5, size = 0.1) +
    labs(x = "log-odds survival",
         y = "density",
         title = "Sampled probability density curves")

p2 <- tibble(sim_tanks = logistic(rnorm(8000, post$a, post$sigma))) %>%
    ggplot(aes(sim_tanks)) +
    geom_density(size = 1, fill = grey, alpha = 0.5) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = expansion(mult = c(0, 0.02))) +
    labs(x = "probability survive",
         y = "density",
         title = "Simulated survival proportions")

p1 | p2
```

- there is uncertainty about both the location $\alpha$ and scale $\sigma$ of the population distribution of log-odds of survival
    * this uncertainty is propagated into the simulated probabilities of survival

## 12.2 Varying effects and the underfitting/overfitting trade-off

- *"Varying intercepts are just regularized estimates, but adaptivelyy regulraized by estimating how diverse the cluster are while estimating the features of each cluster."*
    * varying effect estimates are more accurate estimates of the individual cluster intercepts
- partial pooling helps prevent overfitting and underfitting
    * pooling all of the tanks into a single intercept would make an underfit model
    * having completely separate intercepts for each tank would overfit
- demonstration: simulate tadpole data so we know the true per-pond survival probabilities
    * this is also a demonstration of the important skill of simulation and model validation

### 12.2.1 The model

- we will use the same multilevel binomial model as before (using "ponds" instead of "tanks")

$$
s_i \sim \text{Binomial}(n_i, p_i) \\
\text{logit}(p_i) = \alpha_{\text{pond[i]}} \\
\alpha_\text{pond} \sim \text{Normal}(\alpha, \sigma) \\
\alpha \sim \text{Normal}(0, 1) \\
\sigma \sim \text{HalfCauchy}(0, 1)
$$
- need to assign values for:
    * $\alpha$: the average log-odds of survival for all of the ponds
    * $\sigma$: the standard deviation of the distribution of log-odds of survival among ponds
    * $\alpha_\text{pond}$: the individual pond intercepts
    * $n_i$: the number of tadpoles per pond

### 12.2.2 Assign values to the parameters

- steps in code:
    1. initialize $\alpha$, $\sigma$, number of ponds, number of tadpoles per ponds
    2. use these parameters to generate $\alpha_\text{pond}$
    3. put data into a data frame

```{r}
set.seed(0)

# 1. Initialize top level parameters.
a <- 1.4
sigma <- 1.5
nponds <- 60
ni <- as.integer(rep(c(5, 10, 25, 35), each = 15))

# 2. Sample second level parameters for each pond.
a_pond <- rnorm(nponds, mean = a, sd = sigma)

# 3. Organize into a data frame.
dsim <- tibble(pond = seq(1, nponds), 
               ni = ni,
               true_a = a_pond)
dsim
```
### 12.2.3 Simulate survivors

- simulate the binomial survival process
    * each pond $i$ has $n_i$ potential survivors with probability of survival $p_i$
    * from the model definition (using the logit link function), $p_i$ is:

$$
p_i = \frac{\exp(\alpha_i)}{1 + \exp(\alpha_i)}
$$

```{r}
dsim$si <- rbinom(nponds, 
                  prob = logistic(dsim$true_a), 
                  size = dsim$ni)
```

### 12.2.4 Compute the no-pooling estiamtes

- the estimates from not pooling information across ponds is the same as calculating the proportion of survivors in each pond
    * would get same values if used a dummy variable for each pond and weak priors
- calculate these value and keep on the probability scale

```{r}
dsim$p_nopool <- dsim$si / dsim$ni
```

### 12.2.5 Compute the partial-pooling estimates

- now fit the multilevel model

```{r}
stash("m12_3", {
    m12_3 <- map2stan(
        alist(
            si ~ dbinom(ni, p),
            logit(p) <- a_pond[pond],
            a_pond[pond] ~ dnorm(a, sigma),
            a ~ dnorm(0, 1),
            sigma ~ dcauchy(0, 1)
        ),
        data = dsim,
        iter = 1e4,
        warmup = 1000
    )
})

precis(m12_3, depth = 2)
```

- compute the predicted survival proportions

```{r}
estimated_a_pond <- as.numeric(coef(m12_3)[1:nponds])
dsim$p_partpool <- logistic(estimated_a_pond)
```

- compute known survival proportions from the real $\alpha_\text{pond}$ values

```{r}
dsim$p_true <- logistic(dsim$true_a)
```

- plot the results and compute error between the estimated and true varying effects

```{r}
dsim %>%
    transmute(nopool_error = abs(p_nopool - p_true),
              partpool_error = abs(p_partpool - p_true),
              pond, ni) %>%
    pivot_longer(-c(pond, ni),
                 names_to = "model_type", values_to = "absolute_error") %>%
    group_by(ni, model_type) %>%
    mutate(avg_error = mean(absolute_error)) %>%
    ungroup() %>%
    ggplot(aes(x = pond, y = absolute_error)) +
    facet_wrap(~ ni, scales = "free_x", nrow = 1) +
    geom_line(aes(y = avg_error, color = model_type, group = model_type), size = 1.5, alpha = 0.7) +
    geom_line(aes(group = factor(pond)), color = light_grey, size = 0.8) +
    geom_point(aes(color = model_type)) +
    scale_color_brewer(palette = "Dark2") +
    theme(legend.position = c(0.9, 0.7)) +
    labs(x = "pond number",
         y = "absolute error",
         color = "model type",
         title = "Comparing the error between estimates from amnesiac and multilevel models")
```

- interpretation:
    * both models perform better with larger ponds becasue more data
    * the partial pooling model performs better, on average, than the no pooling model

## 12.3 More than one type of cluster
