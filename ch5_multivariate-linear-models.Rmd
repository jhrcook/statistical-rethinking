---
title: "Chapter 4. Linear Models"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(glue)
library(conflicted)
library(rethinking)
library(patchwork)
library(tidyverse)

conflict_prefer("filter", "dplyr")
theme_set(theme_minimal())
source_scripts()
set.seed(0)
```

- correlation is very common in the real world
- *multivariate regression* is using more than one predictor variable to model an outcome
- reasons to use multivariate regression:
    * a way of "controlling" for confounding variables
    * multiple causation
    * interaction between variables
- this chapter focuses on two thing multivariate models can help with:
    * revealing spurious correlations
    * revealing important correlations masked by hidden correlations of other variables
- this chapter will also discuss:
    * multicolinearity
    * categorical variables

## 5.1 Spurious association

- example: correlation between divorce rate and marriage rate
    * need to be married to get divorced
    * perhaps higher rates of marriage indicate that marriage is more important, leading to fewer divorces
    * another predictor: median age at marriage
- we can fit a model of median age predicting divorce rate
    * this is the same as in the previous chapter
    * $D_i$: divorce rate for state $i$; $A_i$: median age at marriage in state $i$

$$
D_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta_A A_i \\
\alpha \sim \text{Normal}(10, 10) \\
\beta_A \sim \text{Normal}(0, 1) \\
\sigma \sim \text{Uniform}(0, 10)
$$

```{r}
# Load data.
data("WaffleDivorce")
d <- WaffleDivorce

# Stadardize predictor.
d$MedianAgeMarriage_std <- (d$MedianAgeMarriage - mean(d$MedianAgeMarriage)) / sd(d$MedianAgeMarriage)

m5_1 <- quap(
    alist(
        Divorce ~ dnorm(mu, sigma),
        mu <- a + bA * MedianAgeMarriage_std,
        a ~ dnorm(10, 10),
        bA ~ dnorm(0, 1),
        sigma ~ dunif(0, 10)
    ),
    data = d
)

summary(m5_1)
```

```{r}
mam_seq <- seq(-3, 3.5, length.out = 30)

mu <- link(m5_1, data = data.frame(MedianAgeMarriage_std = mam_seq))
mu_map <- apply(mu, 2, chainmode)
m5_1_pred <- apply(mu, 2, PI) %>%
    t() %>%
    as.data.frame() %>%
    set_names(c("pi_5", "pi_94")) %>%
    as_tibble() %>%
    mutate(mam_std = mam_seq,
           mu_map = mu_map)

d %>%
    ggplot() +
    geom_point(aes(x = MedianAgeMarriage_std, y = Divorce)) +
    geom_ribbon(data = m5_1_pred,
                aes(x = mam_std, ymin = pi_5, ymax = pi_94),
                fill = "black", alpha = 0.2, color = NA) +
    geom_line(data = m5_1_pred,
                aes(x = mam_std, y = mu_map),
                color = "blue", alpha = 0.6, lty = 2, size = 1.3)
```

- and we can model the divorce rate on the number of marriages in a state:
    * $R_i$: rate of marriage in state $i$

$$
D_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta_R R_i \\
\alpha \sim \text{Normal}(10, 10) \\
\beta_A \sim \text{Normal}(0, 1) \\
\sigma \sim \text{Uniform}(0, 10)
$$

```{r}
# Stadardize predictor.
d$Marriage_std <- (d$Marriage - mean(d$Marriage)) / sd(d$Marriage)

m5_2 <- quap(
    alist(
        Divorce ~ dnorm(mu, sigma),
        mu <- a + bR * Marriage_std,
        a ~ dnorm(10, 10),
        bR ~ dnorm(0, 1),
        sigma ~ dunif(0, 10)
    ),
    data = d
)

summary(m5_2)
```

- but individual single-variate models cannot tell us which variable is more important or if they cancel each other out
- the question we want to answer: *"What is the predictive value of a variable, once I already know all of the other predictor variables?"*
    * after I know the marriage rate, what additional value is there in also knowing the age at marriage?
    * after I know the age at marriage, what additional value is there in also knowing the marriage rate?

### 5.1.1 Multivariate notation

- the strategy for building a multivariate model:
    1. nominate the predictor variables you want in the linear model of the mean
    2. for each predictor, make a parameter that will measure its association with the outcome
    3. multiply the parameter by the variable and add that term to the linear model
- the formula for our multivariate model example on divorce rate:

$$
D_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta_R R_i + \beta_A A_i \\
\alpha \sim \text{Normal}(10, 10) \\
\beta_A \sim \text{Normal}(0, 1) \\
\beta_R \sim \text{Normal}(0, 1) \\
\sigma \sim \text{Uniform}(0, 10)
$$

- what does $\mu_i = \alpha + \beta_R R_i + \beta_A A_i$ mean:
    * the expected outcome for any state with marriage rate $R_i$ and median age at marriage $A_i$ is the sum of three independent terms
    * $\alpha$ is a constant that every state gets
    * $\beta_R R_i$ is the marriage rate multiplied against a coefficient $\beta_R$ that measures the association between the marriage rate and divorce rate
    * $\beta_A A_i$ is similar to the second term, but for the association with median age at marriage

### 5.1.2 Fitting the model

- we can use the quadratic approximation to fit the model

```{r}
m5_3 <- quap(
    alist(
        Divorce ~ dnorm(mu, sigma),
        mu <- a + bR * Marriage_std + bA * MedianAgeMarriage_std,
        a ~ dnorm(10, 10),
        bA ~ dnorm(0, 1),
        bR ~ dnorm(0, 1),
        sigma ~ dunif(0, 10)
    ),
    data = d
)

summary(m5_3)
```

```{r}
plot(summary(m5_3))
```

- now, the coefficient for the marriage rate predictor is about zero and the coefficient of the median age is confidently below zero
    we can interpret these to mean: *"Once we know the median age at marriage for a state, there is little predictive power in also knowing the rate of marriage in that state."*
- we can make some plots to investigate how the model came to this conclusion

### 5.1.3 Plotting the multivariate posteriors

- we will use three types of interpretive plots
    1. *predictor residual plots*: show the outcome against residual predictor values
    2. *counterfactual plots*: show the implied predictions for imaginary experiments in which the different predictor variables can be changed independently of one another
    3. *posterior prediction plots*: show model-based predictions against raw data, or otherwise display the error in prediction

#### 5.1.3.1 Predictor residual plots

- *predictor variable residual*: the average prediction error when using all other predictor variables to model a predictor of interest
    * plotting this against the outcome shows something like a bivariate regression that has already been "controlled" for all of the other predictors
    * it leaves the variation not expected by the model of the mean of the other predictors
- this is best illustrated by an example:
    * we will model the marriage rate using the median age at marriage

$$
R_i \sim \text{Normal}(\mu_i \sigma) \\
\mu_i = \alpha + \beta A_i \\
\alpha \sim \text{Normal}(0, 10) \\
\beta \sim \text{Normal}(1, 0) \\
\sigma \sim \text{Uniform}(0, 10)
$$

- since we are using centered variables, $\apha$ should be zero

```{r}
m5_4 <- quap(
    alist(
        Marriage_std ~ dnorm(mu, sigma),
        mu <- a + b*MedianAgeMarriage_std,
        a ~ dnorm(0, 10),
        b ~ dnorm(0, 1),
        sigma ~ dunif(0, 10)
    ),
    data = d
)

summary(m5_4)
```

- we then compute the residuals by subtracting the observed marriage rate in each state from the predicted rate when using median age
    * a positive residual means the observed rate was greater than that expected given the median age in that state

```{r}
mu <- coef(m5_4)["a"] + coef(m5_4)["b"] * d$MedianAgeMarriage_std
m_resid <- d$Marriage_std - mu
str(m_resid)
```

```{r}
d %>%
    mutate(mu = mu,
           resid = m_resid,
           resid_diff = mu + m_resid) %>%
    ggplot() +
    geom_linerange(aes(x = MedianAgeMarriage_std, 
                       ymin = mu, ymax = resid_diff),
                   size = 0.8, color = "grey60") +
    geom_point(aes(x = MedianAgeMarriage_std, y = Marriage_std),
               color = "black", size = 2) +
    geom_line(aes(x = MedianAgeMarriage_std, y = mu),
              color = "tomato", size = 1.3, alpha = 0.7) +
    labs(title = "Residual marriage rate estimated using the median age at marriage",
         subtitle = "The red line is the estimate, and the vertical lines are the residuals")
```

- we can then plot these residuals against the divorce rate
    * this is the linear relationship between divorce and marriage rates after "controlling" for median age
    
```{r}
d %>%
    mutate(mu = mu,
           resid = m_resid,
           resid_diff = mu + m_resid) %>%
    ggplot() +
    geom_point(aes(x = resid, y = Divorce),
               color = "black", size = 2) +
    geom_vline(xintercept = 0, lty = 2, color = "dodgerblue", size = 0.9) +
    labs(x = "residual marriage rate",
         title = "Residual marriage rate and Divorce",
         subtitle = "The linear relationship of marriage and divorce rates after correcting for median age at marriage")
```

- we can do the same calculation in the other direction: find the residual of the median age modeled on the rate

```{r}
m5_4_2 <- quap(
    alist(
        MedianAgeMarriage_std ~ dnorm(mu, sigma),
        mu <- a + b*Marriage_std,
        a ~ dnorm(0, 10),
        b ~ dnorm(0, 1),
        sigma ~ dunif(0, 10)
    ),
    data = d
)

summary(m5_4)
```

```{r}
mu <- coef(m5_4_2)["a"] + coef(m5_4_2)["b"] * d$Marriage_std
m_resid <- d$MedianAgeMarriage_std - mu


p1 <- d %>%
    mutate(mu = mu,
           resid = m_resid,
           resid_diff = mu + m_resid) %>%
    ggplot(aes(x = Marriage_std)) +
    geom_linerange(aes(ymin = mu, ymax = resid_diff),
                   size = 0.8, color = "grey60") +
    geom_point(aes(y = MedianAgeMarriage_std),
               color = "black", size = 2) +
    geom_line(aes(y = mu),
              color = "tomato", size = 1.3, alpha = 0.7) +
    labs(title = "Residual median age estimated\nusing the marriage rate",
         subtitle = "The red line is the estimate,\nand the vertical lines are the residuals")

p2 <- d %>%
    mutate(mu = mu,
           resid = m_resid,
           resid_diff = mu + m_resid) %>%
    ggplot() +
    geom_point(aes(x = resid, y = Divorce),
               color = "black", size = 2) +
    geom_vline(xintercept = 0, lty = 2, color = "dodgerblue", size = 0.9) +
    labs(x = "residual median age",
         title = "Residual median age and Divorce",
         subtitle = "The linear relationship of median age\nand divorce after correcting for marriage rate")

p1 | p2
```

- the negative slope of the residual median age vs. Divorce (on the right in the above plot) indicates that the median age contains information even after adjusting for marriage rate

#### 5.1.3.2 Counterfactual plots

- this plot displays the *implied* predictions of the model
    * we can make predictions for inputs that were never seen or are technically impossible
    * e.g.: a high marriage rate and high median age
- the simplest counterfactual plot is to see how predictions change while changing only one predictor
- we will draw two counterfactual plots, one for each predictor

```{r}
# Make new "data" while holding median age constant.
A_avg <- mean(d$MedianAgeMarriage_std)
R_seq <- seq(-3, 3, length.out = 30)
pred_data <- tibble(Marriage_std = R_seq,
                    MedianAgeMarriage_std = A_avg)

# Compute the counterfactual mu values.
mu <- link(m5_3, data = pred_data)
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI) %>% 
    pi_to_df() %>%
    set_names(c("mu_5_pi", "mu_94_pi"))

# Simulate counterfactual divorce outcomes
R_sim <- sim(m5_3, data = pred_data, n = 1e4)
R_pi <- apply(R_sim, 2, PI) %>% 
    pi_to_df() %>%
    set_names(c("Rsim_5_pi", "Rsim_94_pi"))

R_counterfactual <- pred_data %>%
    mutate(mu = mu_mean) %>%
    bind_cols(mu_pi, R_pi)

R_counterfactual %>%
    ggplot(aes(x = Marriage_std)) +
    geom_ribbon(aes(ymin = Rsim_5_pi, ymax = Rsim_94_pi),
                color = NA, fill = "black", alpha = 0.2) +
    geom_ribbon(aes(ymin = mu_5_pi, ymax = mu_94_pi),
                color = NA, fill = "black", alpha = 0.4) +
    geom_line(aes(y = mu), color = "black", size = 1.4) +
    labs(x = "Marriage_std",
         y = "Divorce",
         title = "Counterfactual holding median age constant",
         subtitle = "The line is the mean divorce rate over marriage rate, holding age constant.
The inner ribbon (darker) is the 95% PI for the mean over marriage rate.
The outer ribbon is the 95% PI of the simulated divorce rates.")
```

```{r}
# Make new "data" while holding median age constant.
R_avg <- mean(d$Marriage_std)
A_seq <- seq(-3, 3, length.out = 30)
pred_data <- tibble(Marriage_std = R_avg,
                    MedianAgeMarriage_std = A_seq)

# Compute the counterfactual mu values.
mu <- link(m5_3, data = pred_data)
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI) %>% 
    pi_to_df() %>%
    set_names(c("mu_5_pi", "mu_94_pi"))

# Simulate counterfactual divorce outcomes
A_sim <- sim(m5_3, data = pred_data, n = 1e4)
A_pi <- apply(A_sim, 2, PI) %>% 
    pi_to_df() %>%
    set_names(c("Asim_5_pi", "Asim_94_pi"))

A_counterfactual <- pred_data %>%
    mutate(mu = mu_mean) %>%
    bind_cols(mu_pi, A_pi)

A_counterfactual %>%
    ggplot(aes(x = MedianAgeMarriage_std)) +
    geom_ribbon(aes(ymin = Asim_5_pi, ymax = Asim_94_pi),
                color = NA, fill = "black", alpha = 0.2) +
    geom_ribbon(aes(ymin = mu_5_pi, ymax = mu_94_pi),
                color = NA, fill = "black", alpha = 0.4) +
    geom_line(aes(y = mu), color = "black", size = 1.4) +
    labs(x = "MedianAgeMarriage_std",
         y = "Divorce",
         title = "Counterfactual holding median age constant",
         subtitle = "The line is the mean divorce rate over median age, holding marriage rate constant.
The inner ribbon (darker) is the 95% PI for the mean over median age.
The outer ribbon is the 95% PI of the simulated divorce rates.")
```

- these show 
    * that changing the marriage rate while holding the median age constant, does not cause much change to the predicted rates of divorce
    * but changing the median age, holding the marriage rate constant, does predict substantial effects on divorce rate
- counterfactual plots are contentious because of their small-world nature

#### 5.1.3.3 Posterior prediction plots

- it is important to check the model fit against the observed data:
    1. Did the model fit correctly?
    2. How does the model fail?
        * all models fail in some way; important to find the limitations of the predictions
- begin by simulating predictions, averaging over the posterior

```{r}
# Simulate mu using the original data (by not providing new data)
mu <- link(m5_3)
mu_mean <- apply(mu, 2, mean)
mu_pi <- apply(mu, 2, PI)

# Simulate divorce rates using the original data (by not providing new data)
divorce_sim <- sim(m5_3, n = 1e4)
divorce_pi <- apply(divorce_sim, 2, PI)
```

- for multivariate models, there are many ways to display these simulations
- first, we can plot the predictions against the observed
    * the dashed line represents perfect correlation between the predicted and observed
    * we see that the model under-predicts for states with high divorce rates and over-predicts for those with low rates

```{r}
tibble(mu_mean = mu_mean,
       obs_divorce = d$Divorce,
       state = as.character(d$Loc)) %>%
    mutate(label = ifelse(state %in% c("ID", "UT"), state, NA_character_)) %>%
    bind_cols(pi_to_df(mu_pi)) %>%
    ggplot(aes(x = obs_divorce, y = mu_mean)) +
    geom_pointrange(aes(ymin = x5_percent, ymax = x94_percent)) +
    geom_abline(slope = 1, intercept = 0, lty = 2, alpha = 0.6) +
    ggrepel::geom_text_repel(aes(label = label), family = "Arial") +
    labs(x = "observed divorce",
         y = "predicted divorce")
```

- we can also show the residual of the predictions (the prediction error)
    * we can arrange this plot from least to most prediction error

```{r, fig.height=4, fig.width=3}
tibble(divorce = d$Divorce,
       mu_mean = mu_mean,
       state = as.character(d$Loc)) %>%
    mutate(divorce_resid = divorce - mu_mean,
           state = fct_reorder(state, divorce_resid)) %>%
    bind_cols(pi_to_df(mu_pi)) %>%
    mutate(x5_percent = divorce - x5_percent,
           x94_percent = divorce - x94_percent) %>%
    ggplot(aes(x = divorce_resid, y = state)) +
    geom_pointrange(aes(xmin = x5_percent, xmax = x94_percent)) +
    labs(x = "residual divorce", y = "state")
```

- a third type of plot is to compare the divorce residual against new predictors
    * this can show if there are additional predictors that add information
    * in the following example, I compare the divorce residual against the number of Waffle Houses per capita in the state

```{r}
label_states <- c("ME", "AR", "AL", "MS", "GA", "SC", "ID", "UT")

tibble(divorce = d$Divorce,
       mu_mean = mu_mean,
       waffle_houses = d$WaffleHouses,
       population = d$Population,
       state = as.character(d$Loc)) %>%
    mutate(divorce_resid = divorce - mu_mean,
           waffles_per_capita = waffle_houses / population,
           label = ifelse(state %in% label_states, state, NA_character_)) %>%
    bind_cols(pi_to_df(mu_pi)) %>%
    ggplot(aes(x = waffles_per_capita, y = divorce_resid)) +
    geom_point() +
    geom_smooth(method = "lm", formula = "y ~ x") +
    ggrepel::geom_text_repel(aes(label = label), family = "Arial") +
    labs(x = "Waffle Houses per capita",
         y = "residual divorce")
```

## 5.2 Masked relationship

- another reason to use a multivariate model is to identify influences of multiple factors when none of them show an individual bivariate relationship with the outcome
- for this section, we will use a new data set: the composition of milk across primate species
    * for now, we will use:
        - `kcal_per_g`: Calories per gram of milk (in Cal/g)
        - `mass`: average female body mass (in kg)
        - `neocortex_perc`: percent of total brain mass that is the neocortex

```{r}
data(milk)
d <- janitor::clean_names(milk)
str(d)
```

- the following plots provide a little information about these columns - this was not included in the course

```{r}
d %>%
    select(clade, kcal_per_g, mass, neocortex_perc) %>%
    pivot_longer(-clade, names_to = "name", values_to = "value") %>%
    ggplot(aes(x = clade, y = value, color = clade)) +
    facet_wrap(~ name, nrow = 1, scales = "free") +
    geom_jitter() +
    theme(axis.text.x = element_text(angle = 30, hjust = 1),
          legend.position = "none")
```

```{r}
d %>%
    ggplot(aes(x = kcal_per_g, y = neocortex_perc, color = clade, size = mass)) +
    geom_point()
```

- the first model is just a bivariate regression between kilocalories and neocortex percent
    * we dropped rows with missing values (in neocortex percent)

```{r}
dcc <- d[complete.cases(d), ]
dim(dcc)
```

```{r}
m5_5 <- quap(
    alist(
        kcal_per_g ~ dnorm(mu, sigma),
        mu <- a + bn*neocortex_perc,
        a ~ dnorm(0, 100),
        bn ~ dnorm(0, 1),
        sigma ~ dunif(0, 1)
    ),
    data = dcc
)

precis(m5_5, digits = 4)
```

- the estimate for the coefficient for the neocortex percent is near zero and not very precise
    * the 89% interval is wide on both side of 0
- the next model will use the logarithm of the mother's body mass to predict the kilocalories of the milk
    * use the logarithm because we are curious about changes in magnitude

```{r}
dcc$log_mass <- log(dcc$mass)

m5_6 <- quap(
    alist(
        kcal_per_g ~ dnorm(mu, sigma),
        mu <- a + bm*log_mass,
        a ~ dnorm(0, 100),
        bm ~ dnorm(0, 1),
        sigma ~ dunif(0, 1)
    ),
    data = dcc
)

precis(m5_6)
```

- the mean coefficient is still very small and imprecise (large 89% interval)
- the next model includes both the neocortex percent and mother's mass as predictors of kilocalories

$$
k_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta_n n_i + \beta_m \log(m_i) \\
\alpha \sim \text{Normal}(0, 100) \\
\beta_n \sim \text{Normal}(0, 1) \\
\beta_m \sim \text{Normal}(0, 1) \\
\sigma \sim \text{Uniform}(0, 1)
$$

```{r}
m5_7 <- quap(
    alist(
        kcal_per_g ~ dnorm(mu, sigma),
        mu <- a + bn*neocortex_perc + bm*log_mass,
        a ~ dnorm(0, 100),
        bn ~ dnorm(0, 1),
        bm ~ dnorm(0, 1),
        sigma ~ dunif(0, 1)
    ),
    data = dcc
)

precis(m5_7)
```

- incorporating both predictor variables the estimation association of both with the kilocalories of the milk has increased
    * the posterior mean for the neocortex has increased almost a magnitude and the 89% interval is confidently beyond 0
    * the posterior mean for the log mass is even more negative
- the following plot shows this relationship with the size of the point as the kilocalories of the milk

```{r}
dcc %>%
    ggplot(aes(x = log_mass, y = neocortex_perc, size = kcal_per_g)) +
    geom_point()
```

- we can make counterfactual plots for each variable

```{r}
# Counterfactual holding log mass constant.
avg_log_mass <- mean(dcc$log_mass)
neocortex_seq <- seq(50, 80, length.out = 30)
data_n <- tibble(neocortex_perc = neocortex_seq,
                 log_mass = avg_log_mass)
mu_n <- link(m5_7, data = data_n)
mu_n_mean <- apply(mu_n, 2, mean)
mu_n_pi <- apply(mu_n, 2, PI) %>% 
    pi_to_df()

# Counterfactual holding neocortex percent constant.
avg_neopct <- mean(dcc$neocortex_perc)
logmass_seq <- seq(-2.2, 4.5, length.out = 30)
data_m <- tibble(neocortex_perc = avg_neopct,
                 log_mass = logmass_seq)
mu_m <- link(m5_7, data = data_m)
mu_m_mean <- apply(mu_m, 2, mean)
mu_m_pi <- apply(mu_m, 2, PI) %>% 
    pi_to_df()

# Combine data and plot.
bind_rows(
    tibble(counterfactual = "neocortex percent",
           mu_mean = mu_n_mean,
           x = neocortex_seq) %>%
        bind_cols(mu_n_pi),
    tibble(counterfactual = "log mass",
           mu_mean = mu_m_mean,
           x = logmass_seq) %>%
        bind_cols(mu_m_pi)
) %>%
    ggplot(aes(x = x)) +
    facet_wrap(~ counterfactual, scales = "free", nrow = 1) +
    geom_ribbon(aes(ymin = x5_percent, ymax = x94_percent),
                fill = "black", alpha = 0.3, color = "black", lty = 2) +
    geom_line(aes(y = mu_mean),
              color = "black", lty = 1) +
    labs(title = "Counterfactual plots for model 'm5_7'",
         subtitle = "For each plot, the other variable was set constant",
         x = "variable value",
         y = "kilocalorie energy")
```

- these variables only became useful when used together because they both correlate with the outcome, but one is positive and the other is negative
    * also, these two variables are positively correlated
    * therefore, they tend to "mask" each other until both are used in the model

## 5.3 When adding variables hurts


- why not use all of the available predictors for the model?
    1. *multicolinearity*
    2. *post-treatment bias*
    3. *overfitting*

### 5.3.1 Multicollinear legs

- we start with an example: predicting height using length of a persons leg
    * they are correlated, but problems happen when both legs are included in the model
- we simulate 100 individuals
    * height is sampled from a Gaussian
    * each person gets a simulated proportion of height for their legs, ranging from 0.4 to 0.5
    * the second leg is made from the first with a touch of error

```{r}
# Create data.
N <- 100
height <- rnorm(N, 10, 2)
leg_prop <- runif(N, 0.4, 0.5)
leg_left <- leg_prop * height + rnorm(N, 0, 0.02)
leg_right <- leg_prop * height + rnorm(N, 0, 0.02)
d <- tibble(height, leg_left, leg_right)

# Model.
m5_8 <- quap(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + bl*leg_left + br*leg_right,
        a ~ dnorm(10, 100),
        bl ~ dnorm(2, 10),
        br ~ dnorm(2, 10),
        sigma ~ dunif(0, 10)
    ),
    data = d
)

summary(m5_8)
```

```{r}
plot(summary(m5_8))
```

- the model fit correctly, remember the question that linear regression answers:
    * "What is the value of knowing each predictor, after already knowing all of the other predictors?"
    * for this example: "What is the value of knowing each leg's length, after already knowing the other leg's length?"
- another way of thinking about this is that we have fitted the following formula
    * there are almost an infinite number of pairs of $\beta_1$ and $\beta_2$ that would produce a given $y$ value for a given $x$

$$
y_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta_1 x_i + \beta_2 x_i \\
\mu_i = \alpha + (\beta_1 + \beta_2) x_i = \alpha + \beta_3 x_i
$$

- *when two predictor variables are very strongly correlated, including both in a model may lead to confusion*
- importantly, this model makes fine predictions about height given the length of the left and right legs, it just doesn't say anything about which leg is more important

### 5.3.2 Multicollinear milk
